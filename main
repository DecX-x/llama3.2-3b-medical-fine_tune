{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:20:17.347863Z","iopub.execute_input":"2024-11-26T16:20:17.348254Z","iopub.status.idle":"2024-11-26T16:20:17.615039Z","shell.execute_reply.started":"2024-11-26T16:20:17.348216Z","shell.execute_reply":"2024-11-26T16:20:17.614372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:20:18.232061Z","iopub.execute_input":"2024-11-26T16:20:18.232498Z","iopub.status.idle":"2024-11-26T16:21:45.968230Z","shell.execute_reply.started":"2024-11-26T16:20:18.232470Z","shell.execute_reply":"2024-11-26T16:21:45.967069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:21:45.970092Z","iopub.execute_input":"2024-11-26T16:21:45.970376Z","iopub.status.idle":"2024-11-26T16:22:03.033550Z","shell.execute_reply.started":"2024-11-26T16:21:45.970349Z","shell.execute_reply":"2024-11-26T16:22:03.032798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:22:05.361484Z","iopub.execute_input":"2024-11-26T16:22:05.361882Z","iopub.status.idle":"2024-11-26T16:22:05.366776Z","shell.execute_reply.started":"2024-11-26T16:22:05.361830Z","shell.execute_reply":"2024-11-26T16:22:05.365904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3.2 3B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:22:03.034501Z","iopub.execute_input":"2024-11-26T16:22:03.035009Z","iopub.status.idle":"2024-11-26T16:22:05.359257Z","shell.execute_reply.started":"2024-11-26T16:22:03.034969Z","shell.execute_reply":"2024-11-26T16:22:05.358569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\", \n    model=model_id, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\n\npipe(\"Hey my name is Julien! How are you?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:46:16.477891Z","iopub.execute_input":"2024-11-26T15:46:16.478234Z","iopub.status.idle":"2024-11-26T15:46:27.142892Z","shell.execute_reply.started":"2024-11-26T15:46:16.478205Z","shell.execute_reply":"2024-11-26T15:46:27.142078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = model_id\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3.2-3b-chat-doctor\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:22:05.367742Z","iopub.execute_input":"2024-11-26T16:22:05.367982Z","iopub.status.idle":"2024-11-26T16:22:05.378669Z","shell.execute_reply.started":"2024-11-26T16:22:05.367960Z","shell.execute_reply":"2024-11-26T16:22:05.377816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:22:05.379685Z","iopub.execute_input":"2024-11-26T16:22:05.379924Z","iopub.status.idle":"2024-11-26T16:22:05.389557Z","shell.execute_reply.started":"2024-11-26T16:22:05.379901Z","shell.execute_reply":"2024-11-26T16:22:05.388705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:50:48.571832Z","iopub.execute_input":"2024-11-26T15:50:48.572189Z","iopub.status.idle":"2024-11-26T15:50:55.745111Z","shell.execute_reply.started":"2024-11-26T15:50:48.572158Z","shell.execute_reply":"2024-11-26T15:50:55.744259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.chat_template = None  # Reset the chat template # Assign the new chat template\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:53:17.027103Z","iopub.execute_input":"2024-11-26T15:53:17.027939Z","iopub.status.idle":"2024-11-26T15:53:24.148666Z","shell.execute_reply.started":"2024-11-26T15:53:17.027902Z","shell.execute_reply":"2024-11-26T15:53:24.147908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:53:27.237642Z","iopub.execute_input":"2024-11-26T15:53:27.238490Z","iopub.status.idle":"2024-11-26T15:53:27.733519Z","shell.execute_reply.started":"2024-11-26T15:53:27.238452Z","shell.execute_reply":"2024-11-26T15:53:27.732716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:53:34.583533Z","iopub.execute_input":"2024-11-26T15:53:34.584202Z","iopub.status.idle":"2024-11-26T15:53:41.979115Z","shell.execute_reply.started":"2024-11-26T15:53:34.584154Z","shell.execute_reply":"2024-11-26T15:53:41.978127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:53:50.720700Z","iopub.execute_input":"2024-11-26T15:53:50.721106Z","iopub.status.idle":"2024-11-26T15:53:50.737678Z","shell.execute_reply.started":"2024-11-26T15:53:50.721076Z","shell.execute_reply":"2024-11-26T15:53:50.736840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:54:42.597372Z","iopub.execute_input":"2024-11-26T15:54:42.597726Z","iopub.status.idle":"2024-11-26T15:54:42.627738Z","shell.execute_reply.started":"2024-11-26T15:54:42.597695Z","shell.execute_reply":"2024-11-26T15:54:42.627097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:54:52.147712Z","iopub.execute_input":"2024-11-26T15:54:52.148191Z","iopub.status.idle":"2024-11-26T15:54:53.233767Z","shell.execute_reply.started":"2024-11-26T15:54:52.148152Z","shell.execute_reply":"2024-11-26T15:54:53.233088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T15:55:03.868238Z","iopub.execute_input":"2024-11-26T15:55:03.868603Z","iopub.status.idle":"2024-11-26T16:08:53.745845Z","shell.execute_reply.started":"2024-11-26T15:55:03.868571Z","shell.execute_reply":"2024-11-26T16:08:53.745109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:12:05.383650Z","iopub.execute_input":"2024-11-26T16:12:05.384751Z","iopub.status.idle":"2024-11-26T16:12:19.965951Z","shell.execute_reply.started":"2024-11-26T16:12:05.384715Z","shell.execute_reply":"2024-11-26T16:12:19.964704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:12:38.052442Z","iopub.execute_input":"2024-11-26T16:12:38.052845Z","iopub.status.idle":"2024-11-26T16:13:35.810116Z","shell.execute_reply.started":"2024-11-26T16:12:38.052812Z","shell.execute_reply":"2024-11-26T16:13:35.809227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merging","metadata":{}},{"cell_type":"code","source":"new_model = \"Ellbendls/llama-3.2-3b-chat-doctor\"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.chat_template = None  # Reset the chat template\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmodel = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:27:56.771899Z","iopub.execute_input":"2024-11-26T16:27:56.772710Z","iopub.status.idle":"2024-11-26T16:28:53.170478Z","shell.execute_reply.started":"2024-11-26T16:27:56.772676Z","shell.execute_reply":"2024-11-26T16:28:53.169514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:33:07.521126Z","iopub.execute_input":"2024-11-26T16:33:07.521959Z","iopub.status.idle":"2024-11-26T16:33:13.623793Z","shell.execute_reply.started":"2024-11-26T16:33:07.521923Z","shell.execute_reply":"2024-11-26T16:33:13.622674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"llama-3.2-3b-chat-doctor\")\ntokenizer.save_pretrained(\"llama-3.2-3b-chat-doctor\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:38:11.249213Z","iopub.execute_input":"2024-11-26T16:38:11.249567Z","iopub.status.idle":"2024-11-26T16:38:28.448075Z","shell.execute_reply.started":"2024-11-26T16:38:11.249536Z","shell.execute_reply":"2024-11-26T16:38:28.447190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(\"llama-3.2-3b-chat-doctor\", use_temp_dir=False)\ntokenizer.push_to_hub(\"llama-3.2-3b-chat-doctor\", use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:38:28.449437Z","iopub.execute_input":"2024-11-26T16:38:28.449700Z","iopub.status.idle":"2024-11-26T16:40:52.932224Z","shell.execute_reply.started":"2024-11-26T16:38:28.449673Z","shell.execute_reply":"2024-11-26T16:40:52.931228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert .safetensors to .gguf (for running on ollama, jan etc.)","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:47:09.374417Z","iopub.execute_input":"2024-11-26T16:47:09.375225Z","iopub.status.idle":"2024-11-26T17:01:26.106991Z","shell.execute_reply.started":"2024-11-26T16:47:09.375181Z","shell.execute_reply":"2024-11-26T17:01:26.105900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py /kaggle/working/llama-3.2-3b-chat-doctor/ \\\n    --outfile /kaggle/working/llama-3.2-3b-chat-doctor.gguf \\\n    --outtype f16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:02:06.680789Z","iopub.execute_input":"2024-11-26T17:02:06.681569Z","iopub.status.idle":"2024-11-26T17:02:25.392357Z","shell.execute_reply.started":"2024-11-26T17:02:06.681531Z","shell.execute_reply":"2024-11-26T17:02:25.391519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import HfApi\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)\n\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/llama-3.2-3b-chat-doctor.gguf\",\n    path_in_repo=\"/kaggle/working/llama-3.2-3b-chat-doctor.gguf\",\n    repo_id=\"Ellbendls/llama-3.2-3b-chat-doctor\",\n    repo_type=\"model\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:03:48.449467Z","iopub.execute_input":"2024-11-26T17:03:48.449912Z","iopub.status.idle":"2024-11-26T17:06:19.521892Z","shell.execute_reply.started":"2024-11-26T17:03:48.449879Z","shell.execute_reply":"2024-11-26T17:06:19.521040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}